{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sys import getsizeof\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(s, path = 'log.out', prnt = True):\n",
    "    ''\n",
    "    \n",
    "    f = open(path, \"a\")\n",
    "    f.write('\\n' + s)\n",
    "    if prnt:\n",
    "        print(s)\n",
    "    f.close()\n",
    "\n",
    "def load_node2vec(path: str):\n",
    "    ''\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            vals = line.strip().split()\n",
    "            embeddings_dict[vals[0]] = [float(x) for x in vals[1:]]\n",
    "\n",
    "    print(\"### Node2Vec loaded\")\n",
    "    return embeddings_dict\n",
    "\n",
    "def load_glove(path: str):\n",
    "    ''\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            vals = line.rstrip().split()\n",
    "            embeddings_dict[vals[0]] = [float(x) for x in vals[1:]]\n",
    "\n",
    "    dict_keys = list(embeddings_dict.keys())\n",
    "    print(\"### Glove loaded, vocab size {len(embeddings_dict.keys())}\")\n",
    "    return embeddings_dict, dict_keys\n",
    "\n",
    "def load_entities(path: str):\n",
    "    ''\n",
    "\n",
    "    entities_dict = {}\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            fields = line.strip().split('\\t')\n",
    "            entities_dict[ fields[0] ] = fields[1]\n",
    "    \n",
    "    print('\\nEntities loaded')\n",
    "    return entities_dict\n",
    "\n",
    "def load_relations(path: str):\n",
    "    ''\n",
    "\n",
    "    relations = []\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            relations.append(line.strip())\n",
    "    \n",
    "    print('\\nRelations loaded')\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGDataset(Dataset):\n",
    "    'Dataset'\n",
    "\n",
    "    def __init__(self, path: str,\n",
    "        entities_dict: dict,\n",
    "        descriptions_dict: dict,\n",
    "        node2vec: list,\n",
    "        relations: list,\n",
    "        device,\n",
    "        settings: dict,\n",
    "        embeddings_dict: dict,\n",
    "        dict_keys,\n",
    "        ) -> None:\n",
    "        ''\n",
    "\n",
    "        self.entities_dict = entities_dict\n",
    "        self.descriptions_dict = descriptions_dict\n",
    "        self.relations = relations\n",
    "        self.device = device\n",
    "        self.settings = settings\n",
    "        self.node2vec = node2vec\n",
    "        self.embeddings_dict = embeddings_dict\n",
    "        self.dict_keys = dict_keys\n",
    "        self.stats = {}\n",
    "        self.stats['glove_greedy'] = set()\n",
    "        self.stats['glove_not_found'] = set()\n",
    "\n",
    "        # Read file\n",
    "        # DO NOT FORGET TO Load the graph once and make it global\n",
    "        f = open(path)\n",
    "        file_lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        self.lines = file_lines[: int(len(file_lines) * settings['SAMPLE_SIZE']) ]\n",
    "        print(f'\\nopened {path}' )\n",
    "        self.G = nx.MultiGraph()\n",
    "        for i in range( int( len(self.lines) ) ):\n",
    "            items = self.lines[i].strip().split('\\t')\n",
    "\n",
    "            # Triple has 3 only\n",
    "            assert len(items) == 3\n",
    "\n",
    "            items = [p.strip() for p in items]\n",
    "\n",
    "            head = items[0].strip()\n",
    "            tail = items[2].strip()\n",
    "            rel = items[1].strip()\n",
    "            self.G.add_node(head)\n",
    "            self.G.add_node(tail)\n",
    "            self.G.add_edge(head, tail, name = rel)\n",
    "        \n",
    "        write_log(str(self.G))\n",
    "        \n",
    "    def gettext(self, index):\n",
    "        ''\n",
    "\n",
    "        fields = self.lines[index].strip().split('\\t')\n",
    "        head = self.entities_dict[fields[0]]\n",
    "        tail = self.entities_dict[fields[2]]\n",
    "        rel = fields[1]\n",
    "        return head, rel, tail, fields[0],fields[2]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        ''\n",
    "\n",
    "        return len(self.lines)\n",
    "    \n",
    "    def get_glove_embedding(self, word):\n",
    "        'Gets the embeddings of a word from Glove vectors'\n",
    "        \n",
    "        wrd = str(word).lower()\n",
    "        if wrd in self.embeddings_dict:\n",
    "            return self.embeddings_dict[wrd]\n",
    "        \n",
    "        # Greedy largest chunk\n",
    "        lngth = len(wrd)\n",
    "        for i in range(lngth - 1, 2, -1):\n",
    "            for j in range(lngth - i + 1):\n",
    "                sub_wrd = wrd[j:j + i]\n",
    "                if sub_wrd in self.embeddings_dict:\n",
    "                    return self.embeddings_dict[sub_wrd]\n",
    "        \n",
    "        char_embeddings = [\n",
    "            self.embeddings_dict.setdefault(c, self.embeddings_dict[self.dict_keys[ord(c) % 100]]) for c in wrd\n",
    "        ]\n",
    "        averaged_embeddings = []\n",
    "        for i in range(self.settings['DIMENSIONS']):\n",
    "            tot = 0\n",
    "            for j in range(len(char_embeddings)):\n",
    "                tot += char_embeddings[j][i]\n",
    "            averaged_embeddings.append(tot / self.settings['DIMENSIONS'])\n",
    "\n",
    "        return averaged_embeddings\n",
    "    \n",
    "    def get_ent_embeddings(self, ent: str, is_head = False, use_description = False):\n",
    "        ''\n",
    "\n",
    "        embeddings = []\n",
    "        entity_text = self.entities_dict[ent]\n",
    "\n",
    "        # use description\n",
    "        if use_description:\n",
    "            entity_text = self.descriptions_dict[ent]\n",
    "\n",
    "        words = entity_text.split()\n",
    "\n",
    "        type_vector = [-1 * self.settings['VECTOR_VALUE']]\n",
    "        if is_head:\n",
    "            type_vector = [self.settings['VECTOR_VALUE']]\n",
    "\n",
    "        # Check if longer than allowed\n",
    "        max_size = self.settings['padding'] // 2\n",
    "        if len(words) > max_size:\n",
    "            for j in range( max_size - 1 ):\n",
    "                embeddings.append( self.get_glove_embedding(words[j]) + type_vector  )\n",
    "            \n",
    "            remaining_embeddings = []\n",
    "            for j in range( max_size - 1, len(words) ):\n",
    "                remaining_embeddings.append( self.get_glove_embedding(words[j]) )\n",
    "            \n",
    "            embeddings.append( np.mean(remaining_embeddings, axis=0).tolist() + type_vector )\n",
    "        \n",
    "        else:\n",
    "            for j in range( len(words) ):\n",
    "                embeddings.append( self.get_glove_embedding(words[j]) + type_vector  )\n",
    "\n",
    "        if True: #N2V\n",
    "            embeddings.append(self.node2vec[ent] + type_vector)\n",
    "            \n",
    "        # fill\n",
    "        if len(embeddings) < self.settings['padding'] // 2 + 1:\n",
    "            filling = [((self.settings['DIMENSIONS'] + 1) * [0.0])] * (self.settings['padding'] // 2 + 1 - len(embeddings))\n",
    "            embeddings.extend(filling)\n",
    "        \n",
    "                    \n",
    "        # if is_head: # separator\n",
    "        #     embeddings.append( self.settings['DIMENSIONS'] * [-1] )        \n",
    "            \n",
    "        return embeddings\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ''\n",
    "\n",
    "        fields = self.lines[index].strip().split('\\t')\n",
    "        rel = fields[1]\n",
    "\n",
    "        assert len(fields) == 3\n",
    "\n",
    "        # Prepare Y label\n",
    "        relations_tagged = [0.0] * len(self.relations)\n",
    "        rel_index = self.relations.index(rel)\n",
    "        relations_tagged[ rel_index ] = 1.0\n",
    "\n",
    "        embeddings = []\n",
    "        head_embeddings = self.get_ent_embeddings(fields[0], is_head = True, )\n",
    "        tail_embeddings = self.get_ent_embeddings(fields[2], is_head = False,  )\n",
    "        embeddings.extend(head_embeddings)\n",
    "        embeddings.extend(tail_embeddings)\n",
    "\n",
    "\n",
    "        return torch.Tensor(embeddings),torch.Tensor(relations_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spotintelligence.com/2023/01/31/self-attention/\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):  \n",
    "    def __init__(self,output_size, settings):\n",
    "        ''\n",
    "        \n",
    "        super(MyModel, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM((settings['DIMENSIONS'] + 1), settings['lstm_hidden_size'] // 2,\n",
    "                                num_layers = settings['lstm_layers'], bidirectional=True, batch_first = True)\n",
    "        \n",
    "        self.attn = SelfAttention(settings['lstm_hidden_size'])\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dropout = torch.nn.Dropout(settings['dropout'])\n",
    "        self.last = torch.nn.Linear(settings['lstm_hidden_size'] * (settings['padding'] + 2), output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.last(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(device, relations, settings, training_generator, validation_generator, verbose):\n",
    "    ''\n",
    "\n",
    "    mymodel = MyModel(output_size = len(relations), settings = settings)\n",
    "    # loss_f = torch.nn.CrossEntropyLoss()\n",
    "    loss_f = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(mymodel.parameters(), lr = settings['lr'], )\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, gamma=settings['decay'], step_size= settings['stepping'])\n",
    "    mymodel.to(device)\n",
    "\n",
    "    # Print settings\n",
    "    for k, v in settings.items():\n",
    "        write_log(f'{k:<25} {v}')\n",
    "    \n",
    "    v_loss = 1_000_000\n",
    "    no_change_counter = 1\n",
    "    for epoch in range(settings['EPOCHS']):\n",
    "        print(f'\\nEpoch {epoch + 1}\\n-------------------------------')\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        mymodel.train()\n",
    "        loop = tqdm(training_generator, disable = not verbose)\n",
    "        losses = []\n",
    "\n",
    "        # Loop over batches in an epoch using DataLoader\n",
    "        for _, data in enumerate(loop):\n",
    "            data[0] = data[0].to(device)\n",
    "            data[1] = data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predy = mymodel(data[0])\n",
    "            loss = loss_f(predy, data[1])\n",
    "            loss.backward()       \n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        train_loss = sum(losses) / len(losses)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Loop over batches in an epoch using DataLoader\n",
    "        v_losses = []\n",
    "        mymodel.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for _, data in enumerate(validation_generator):\n",
    "                data[0] = data[0].to(device)\n",
    "                data[1] = data[1].to(device)\n",
    "                predy =  mymodel(data[0])\n",
    "                loss = loss_f(predy, data[1])\n",
    "                v_losses.append(loss)\n",
    "\n",
    "        v_loss_epoch = sum(v_losses) / len(v_losses)\n",
    "        write_log(f'lr {lr:8f} train loss {train_loss:.8f} val loss {v_loss_epoch:.8f}')\n",
    "\n",
    "        if v_loss - v_loss_epoch > 0.00001 :\n",
    "            v_loss = v_loss_epoch\n",
    "            no_change_counter = 0\n",
    "            torch.save(mymodel.state_dict(), 'chkpnt.pt')\n",
    "        elif no_change_counter > settings['PATIENCE'] - 1:\n",
    "            break\n",
    "        else:\n",
    "            no_change_counter += 1\n",
    "        \n",
    "        scheduler.step()    \n",
    "    \n",
    "    mymodel = MyModel(output_size = len(relations), settings = settings)\n",
    "    mymodel.to(device)\n",
    "    mymodel.load_state_dict(torch.load('chkpnt.pt'))\n",
    "    return mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_generator, device, verbose, settings, test_set):\n",
    "    ''\n",
    "\n",
    "    print(f'\\nTesting\\n-------------------------------')\n",
    "\n",
    "    ranks = []\n",
    "    ranks_filtered = []\n",
    "    reciprocal_ranks = []\n",
    "    reciprocal_ranks_filtered = []\n",
    "    ranks_filtered = []\n",
    "    hits = []\n",
    "    hits_filtered = []\n",
    "    for i in range(10):\n",
    "            hits.append([])\n",
    "            hits_filtered.append([])\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(test_generator, disable = not verbose)\n",
    "        for id, data in enumerate(loop):\n",
    "            data[0] = data[0].to(device)\n",
    "            data[1] = data[1].to(device)\n",
    "            y_pred = model(data[0])\n",
    "            for i, item in enumerate(y_pred):\n",
    "                item_id = id * settings['BATCH_SIZE'] + i\n",
    "                h, r, t, e1, e2 = test_set.gettext(item_id)\n",
    "                gold_index = torch.argmax(data[1][i])\n",
    "                indices = torch.argsort(item, descending = True)\n",
    "                # get all the relations for this specific triple\n",
    "                rank = (indices==gold_index).nonzero().item()\n",
    "\n",
    "                # check rank\n",
    "                if rank > 10:\n",
    "                    # request input for description\n",
    "                    embeddings = []\n",
    "                    head_embeddings = test_set.get_ent_embeddings(e1, is_head = True, use_description = True )\n",
    "                    tail_embeddings = test_set.get_ent_embeddings(e2, is_head = False, use_description = True )\n",
    "                    embeddings.extend(head_embeddings)\n",
    "                    embeddings.extend(tail_embeddings)\n",
    "                    y_pred = model( torch.Tensor([embeddings]).to(device) )\n",
    "                    gold_index2 = torch.argmax(data[1][i])\n",
    "                    indices2 = torch.argsort(y_pred[0], descending = True)\n",
    "                    # get all the relations for this specific triple\n",
    "                    new_rank = (indices2==gold_index2).nonzero().item()\n",
    "                    if new_rank < rank:\n",
    "                        rank = new_rank\n",
    "                        indices = indices2\n",
    "                        gold_index = gold_index2\n",
    "\n",
    "                ranks.append(rank + 1)\n",
    "                reciprocal_ranks.append(1/(rank + 1))\n",
    "                \n",
    "\n",
    "                # filter work\n",
    "                filter_rank = rank\n",
    "\n",
    "                # get higher predicted relations\n",
    "                # indices_list = indices.view(-1)\n",
    "                indices_list = indices.tolist()\n",
    "                higher_rels = indices_list[: indices_list.index(gold_index) ]\n",
    "                \n",
    "                # get gold relations for the triple\n",
    "                triple = test_set.gettext(item_id)\n",
    "                key = triple[3] + '_' + triple[4]\n",
    "                rel_list = triples[key]\n",
    "\n",
    "                # loop higher rels\n",
    "                for j, rel_id in enumerate(higher_rels):\n",
    "                    if rel_id in rel_list:\n",
    "                        filter_rank -= 1\n",
    "                \n",
    "                ranks_filtered.append(filter_rank + 1)\n",
    "                reciprocal_ranks_filtered.append(1/(filter_rank + 1))\n",
    "\n",
    "                # Hits work\n",
    "                for hits_level in range(10):\n",
    "                    if rank <= hits_level:\n",
    "                        hits[hits_level].append(1.0)\n",
    "                    else:\n",
    "                        hits[hits_level].append(0.0)\n",
    "                    \n",
    "                    if filter_rank <= hits_level:\n",
    "                        hits_filtered[hits_level].append(1.0)\n",
    "                    else:\n",
    "                        hits_filtered[hits_level].append(0.0)\n",
    "                    \n",
    "                    if rank > 10 and hits_level == 9:\n",
    "                        write_log(str(rank) + '^' +str(item_id) + '^' + h + '^' + r + '^' + t + '^' + e1 + '^' + e2,'failures.out', False)\n",
    "\n",
    "    write_log(f'\\n{\"MR\":<15} {np.mean(ranks):.4f}')\n",
    "    write_log(f'{\"MR Filtered\":<15} {np.mean(ranks_filtered):.4f}')\n",
    "    write_log(f'\\n{\"MRR\":<15} {np.mean(reciprocal_ranks):.4f}')\n",
    "    write_log(f'{\"MRR Filtered\":<15} {np.mean(reciprocal_ranks_filtered):.4f}')\n",
    "    for i in [0,1,2,4,9]:\n",
    "        # write_log('Raw Hits @{0}: {1}'.format(i+1, np.mean(hits[i])))\n",
    "        write_log('')\n",
    "        write_log(f'Raw Hits           {i + 1:<3}: {np.mean(hits[i]):<5.4f}')\n",
    "        write_log(f'Filtered Hits  {i + 1:<3}: {np.mean(hits_filtered[i]):<5.4f}')\n",
    "        # write_log('Raw Filtered Hits @{0}: {1}'.format(i+1, np.mean(hits_filtered[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations(head, tail, relations, settings, device, training_set, entities_dict):\n",
    "    print(entities_dict[head],',',entities_dict[tail])\n",
    "    mymodel = MyModel(output_size = len(relations), settings = settings)\n",
    "    mymodel.to(device)\n",
    "    mymodel.load_state_dict(torch.load('chkpnt.pt'))\n",
    "    mymodel.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        head_embeddings = training_set.get_ent_embeddings(head, is_head = True, )\n",
    "        tail_embeddings = training_set.get_ent_embeddings(tail, is_head = False,  )\n",
    "        embeddings.extend(head_embeddings)\n",
    "        embeddings.extend(tail_embeddings)\n",
    "        embeddings = [embeddings]\n",
    "        embeddings = torch.Tensor(embeddings)\n",
    "        embeddings = embeddings.to(device)\n",
    "        result = mymodel(embeddings)\n",
    "        for index, rel in enumerate(relations):\n",
    "            # print(rel, result[0][index])\n",
    "            print(f'{rel:>100} \\t {result[0][index].item():.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "\n",
    "settings = {\n",
    "    'SAMPLE_SIZE' :            0.001,\n",
    "    'EPOCHS' :                 2,\n",
    "    'VECTOR_VALUE' :           1,\n",
    "    'DIMENSIONS' :             50,\n",
    "    'PATIENCE' :               2,\n",
    "    'BATCH_SIZE' :             32,\n",
    "    'lstm_hidden_size' :       400,\n",
    "    'lstm_layers' :            2,\n",
    "    'dropout' :                0.2,\n",
    "    'lr' :                     1e-3,\n",
    "    'decay' :                  0.35,\n",
    "    'padding' :                20 + 20,\n",
    "    'stepping' :               1,\n",
    "    'n2v_path' :               'n2v/n2v_embeddings50.csv',\n",
    "    # 'n2v_path' :               'n2v/n2v_dimensions_300_walk_length_50_walks_50_window_10.csv',\n",
    "    'entities_path' :          'data/FB15K/entity2text.txt',\n",
    "    'descriptions_path' :          'data/FB15K/entity2textlong.txt',\n",
    "    'relations_path' :         'data/FB15K/relations.txt'\n",
    "}\n",
    "settings['glove_path'] =       'glove.6B.' + str(settings['DIMENSIONS']) + 'd.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "embeddings_dict, dict_keys = load_glove(settings['glove_path'])\n",
    "relations = load_relations(settings['relations_path'])\n",
    "entities_dict = load_entities(settings['entities_path'])\n",
    "descriptions_dict = load_entities(settings['descriptions_path'])\n",
    "node2vec = load_node2vec(settings['n2v_path'])\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "write_log('\\ndevice ' + str(device))\n",
    "\n",
    "# Generators\n",
    "training_set = KGDataset(path = 'data/FB15K/train.tsv', entities_dict = entities_dict, descriptions_dict = descriptions_dict , relations = relations, device=device, settings=settings, node2vec = node2vec, embeddings_dict = embeddings_dict, dict_keys = dict_keys,)\n",
    "training_generator = DataLoader(training_set, batch_size = settings['BATCH_SIZE'], worker_init_fn=seed_worker, generator=g,)\n",
    "\n",
    "validation_set = KGDataset(path = 'data/FB15K/dev.tsv', entities_dict = entities_dict, descriptions_dict = descriptions_dict , relations = relations, device=device, settings=settings, node2vec = node2vec, embeddings_dict = embeddings_dict, dict_keys = dict_keys,)\n",
    "validation_generator = DataLoader(validation_set, batch_size = settings['BATCH_SIZE'], worker_init_fn=seed_worker, generator=g,)\n",
    "\n",
    "test_set = KGDataset(path = 'data/FB15K/test.tsv', entities_dict = entities_dict, descriptions_dict = descriptions_dict , relations = relations, device=device, settings=settings, node2vec = node2vec, embeddings_dict = embeddings_dict, dict_keys = dict_keys,)\n",
    "test_generator = DataLoader(test_set, batch_size = settings['BATCH_SIZE'], worker_init_fn=seed_worker, generator=g,)\n",
    "\n",
    "# delete unnecessary\n",
    "\n",
    "print('\\ninput shape' + str(training_set[0][0].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train_model(device, relations, settings, training_generator, validation_generator, verbose = True)\n",
    "\n",
    "triples = {}\n",
    "for i, _ in enumerate(training_set):\n",
    "    item = training_set.gettext(i)\n",
    "    key = item[3] + '_' + item[4]\n",
    "    rel_list = triples.setdefault(key, [])\n",
    "    rel_index = training_set.relations.index(item[1])\n",
    "    rel_list.append(rel_index)\n",
    "    triples[key] = rel_list\n",
    "for i, _ in enumerate(validation_set):\n",
    "    item = validation_set.gettext(i)\n",
    "    key = item[3] + '_' + item[4]\n",
    "    rel_list = triples.setdefault(key, [])\n",
    "    rel_index = validation_set.relations.index(item[1])\n",
    "    rel_list.append(rel_index)\n",
    "    triples[key] = rel_list\n",
    "for i, _ in enumerate(test_set):\n",
    "    item = test_set.gettext(i)\n",
    "    key = item[3] + '_' + item[4]\n",
    "    rel_list = triples.setdefault(key, [])\n",
    "    rel_index = test_set.relations.index(item[1])\n",
    "    rel_list.append(rel_index)\n",
    "    triples[key] = rel_list\n",
    "\n",
    "\n",
    "# evaluate(model, test_generator, device, verbose = True, settings=settings, test_set=test_set)\n",
    "\n",
    "\n",
    "# tune\n",
    "if True:\n",
    "    for lr in [ 1e-3, 8e-3,]:        \n",
    "        for l1 in [ 400]:\n",
    "            for l2 in [ 2]:\n",
    "                for d in [0.15, ]:\n",
    "                    for dd in [0.5, 0.35, 0.25]:\n",
    "                        for st in [1, ]:\n",
    "                            tune_settings = settings\n",
    "                            tune_settings['lr'] = lr\n",
    "                            tune_settings['lstm_hidden_size'] = l1\n",
    "                            tune_settings['lstm_layers'] = l2\n",
    "                            tune_settings['dropout'] = d\n",
    "                            tune_settings['decay'] = dd\n",
    "                            tune_settings['stepping'] = st\n",
    "                            model = train_model(device, relations, tune_settings, training_generator, validation_generator, verbose = False)\n",
    "                            evaluate(model, test_generator, device, verbose = False, settings=settings, test_set=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /m/0x67\t/people/ethnicity/people\t/m/0411q\n",
    "# get_relations('/m/0x67', '/m/0411q', relations, settings, device, training_set, entities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
